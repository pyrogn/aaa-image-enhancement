{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8230846,"sourceType":"datasetVersion","datasetId":4881182}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Надо научиться это запускать. Или на tensorflow попробовать.","metadata":{}},{"cell_type":"markdown","source":"# https://github.com/google-research/maxim","metadata":{}},{"cell_type":"markdown","source":"# https://github.com/vztu/maxim-pytorch/tree/main/maxim_pytorch","metadata":{}},{"cell_type":"code","source":"!pip install einops","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n#convert pretrained Jax params of MAXIM to Pytorch\nimport argparse\nimport collections\nimport io\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport tensorflow as tf\n# -*- coding: utf-8 -*-\nimport einops\nimport numpy as np\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\n\n\ndef nearest_downsample(x, ratio):\n    n,c,h,w = x.shape\n    new_h = int(h * ratio)\n    new_w = int(w * ratio)\n    h_index = np.floor((np.arange(new_h)+0.5) / ratio)\n    w_index = np.floor((np.arange(new_w)+0.5) / ratio)\n    out = x[:,:,h_index,:]\n    out = out[:,:,:,w_index]\n    return out\n\nclass Layer_norm_process(nn.Module):  #n, h, w, c\n    def __init__(self, c, eps=1e-6):\n        super().__init__()\n        self.beta = torch.nn.Parameter(torch.zeros(c), requires_grad=True)\n        self.gamma = torch.nn.Parameter(torch.ones(c), requires_grad=True)\n        self.eps = eps\n    def forward(self, feature):\n        var_mean = torch.var_mean(feature, dim=-1, unbiased=False)\n        mean = var_mean[1]\n        var = var_mean[0]\n        # layer norm process\n        feature = (feature - mean[..., None]) / torch.sqrt(var[..., None] + self.eps)\n        gamma = self.gamma.expand_as(feature)\n        beta = self.beta.expand_as(feature)\n        feature = feature * gamma + beta\n        return feature\n\ndef block_images_einops(x, patch_size):  #n, h, w, c\n  \"\"\"Image to patches.\"\"\"\n  batch, height, width, channels = x.shape\n  grid_height = height // patch_size[0]\n  grid_width = width // patch_size[1]\n  x = einops.rearrange(\n      x, \"n (gh fh) (gw fw) c -> n (gh gw) (fh fw) c\",\n      gh=grid_height, gw=grid_width, fh=patch_size[0], fw=patch_size[1])\n  return x\n\n\ndef unblock_images_einops(x, grid_size, patch_size):\n  \"\"\"patches to images.\"\"\"\n  x = einops.rearrange(\n      x, \"n (gh gw) (fh fw) c -> n (gh fh) (gw fw) c\",\n      gh=grid_size[0], gw=grid_size[1], fh=patch_size[0], fw=patch_size[1])\n  return x\n\n\nclass UpSampleRatio_4(nn.Module):  #input shape: n,c,h,w.    c-->4c\n    \"\"\"Upsample features given a ratio > 0.\"\"\"\n    def __init__(self, features,b=0, ratio=1., use_bias=True):\n        super().__init__()\n        self.features = features\n        self.ratio = ratio\n        self.bias = use_bias\n        self.Conv_0 = nn.Conv2d(self.features,4*self.features,kernel_size=(1,1),stride=1,bias=self.bias)\n    def forward(self, x):\n        n,c,h,w = x.shape\n        x = F.interpolate(x, size=(int(h * self.ratio), int(w * self.ratio)), mode='bilinear', antialias=True)\n        x = self.Conv_0(x)\n        return x\n    \nclass UpSampleRatio_2(nn.Module):  #input shape: n,c,h,w.    c-->2c\n    \"\"\"Upsample features given a ratio > 0.\"\"\"\n    def __init__(self, features,b=0, ratio=1., use_bias=True):\n        super().__init__()\n        self.features = features\n        self.ratio = ratio\n        self.bias = use_bias\n        self.Conv_0 = nn.Conv2d(self.features,2*self.features,kernel_size=(1,1),stride=1,bias=self.bias)\n    def forward(self, x):\n        n,c,h,w = x.shape\n        x = F.interpolate(x, size=(int(h * self.ratio), int(w * self.ratio)), mode='bilinear', antialias=True)\n        x = self.Conv_0(x)\n        return x\n    \nclass UpSampleRatio(nn.Module):  #input shape: n,c,h,w.    c-->c\n    \"\"\"Upsample features given a ratio > 0.\"\"\"\n    def __init__(self, features,b=0, ratio=1., use_bias=True):\n        super().__init__()\n        self.features = features\n        self.ratio = ratio\n        self.bias = use_bias\n        self.Conv_0 = nn.Conv2d(self.features,self.features,kernel_size=(1,1),stride=1,bias=self.bias)\n    def forward(self, x):\n        x = self.Conv_0(x)\n        return x\n\nclass UpSampleRatio_1_2(nn.Module):  #input shape: n,c,h,w.    c-->c/2\n    \"\"\"Upsample features given a ratio > 0.\"\"\"\n    def __init__(self, features,b=0, ratio=1., use_bias=True):\n        super().__init__()\n        self.features = features\n        self.ratio = ratio\n        self.bias = use_bias\n        self.Conv_0 = nn.Conv2d(self.features,self.features//2,kernel_size=(1,1),stride=1,bias=self.bias)\n    def forward(self, x):\n        n,c,h,w = x.shape\n        x = F.interpolate(x, size=(int(h * self.ratio), int(w * self.ratio)), mode='bilinear', antialias=True)\n        x = self.Conv_0(x)\n        return x\n    \nclass UpSampleRatio_1_4(nn.Module):  #input shape: n,c,h,w.    c-->c/4\n    \"\"\"Upsample features given a ratio > 0.\"\"\"\n    def __init__(self, features,b=0, ratio=1., use_bias=True):\n        super().__init__()\n        self.features = features\n        self.ratio = ratio\n        self.bias = use_bias\n        self.Conv_0 = nn.Conv2d(self.features,self.features//4,kernel_size=(1,1),stride=1,bias=self.bias)\n    def forward(self, x):\n        n,c,h,w = x.shape\n        x = F.interpolate(x, size=(int(h * self.ratio), int(w * self.ratio)), mode='bilinear', antialias=True)\n        x = self.Conv_0(x)\n        return x\n\n\nclass BlockGatingUnit(nn.Module):  #input shape: n (gh gw) (fh fw) c\n    \"\"\"A SpatialGatingUnit as defined in the gMLP paper.\n    The 'spatial' dim is defined as the second last.\n    If applied on other dims, you should swapaxes first.\n    \"\"\"\n    def __init__(self, c, n, use_bias=True):\n        super().__init__()\n        self.c = c\n        self.n = n\n        self.use_bias = use_bias\n        self.Dense_0 = nn.Linear(self.n, self.n, self.use_bias)\n        self.intermediate_layernorm = Layer_norm_process(self.c//2)\n    def forward(self, x):\n        c = x.size(-1)\n        c = c // 2  #split size\n        u, v  = torch.split(x, c, dim=-1)\n        v = self.intermediate_layernorm(v)\n        v = v.permute(0, 1, 3, 2)  #n, (gh gw), c/2, (fh fw)\n        v = self.Dense_0(v)  #apply fc on the last dimension (fh fw)\n        v = v.permute(0, 1, 3, 2)  #n (gh gw) (fh fw) c/2\n        return u* (v + 1.)\n    \nclass GridGatingUnit(nn.Module):  #input shape: n (gh gw) (fh fw) c\n    \"\"\"A SpatialGatingUnit as defined in the gMLP paper.\n    The 'spatial' dim is defined as the second.\n    If applied on other dims, you should swapaxes first.\n    \"\"\"\n    def __init__(self, c, n, use_bias=True):\n        super().__init__()\n        self.c = c\n        self.n = n\n        self.use_bias = use_bias\n        self.intermediate_layernorm = Layer_norm_process(self.c//2)\n        self.Dense_0 = nn.Linear(self.n, self.n, self.use_bias)\n    def forward(self, x):\n        c = x.size(-1)\n        c = c // 2  #split size\n        u, v  = torch.split(x, c, dim=-1)\n        v = self.intermediate_layernorm(v)\n        v = v.permute(0, 3, 2, 1)  #n, c/2, (fh fw) (gh gw)\n        v = self.Dense_0(v)  #apply fc on the last dimension (gh gw)\n        v = v.permute(0, 3, 2, 1)  #n (gh gw) (fh fw) c/2\n        return u* (v + 1.)\n\n\nclass GridGmlpLayer(nn.Module):  #input shape: n, h, w, c\n    \"\"\"Grid gMLP layer that performs global mixing of tokens.\"\"\"\n    def __init__(self, grid_size, num_channels, use_bias=True, factor=2, dropout_rate=0):\n        super().__init__()\n        self.grid_size = grid_size\n        self.gh = grid_size[0]\n        self.gw = grid_size[1]\n        self.num_channels = num_channels\n        self.use_bias = use_bias\n        self.factor = factor\n        self.dropout_rate = dropout_rate\n        self.LayerNorm = Layer_norm_process(self.num_channels)\n        self.in_project = nn.Linear(self.num_channels, self.num_channels*self.factor, self.use_bias)   #c->c*factor\n        self.gelu = nn.GELU(approximate='tanh')\n        self.GridGatingUnit = GridGatingUnit(self.num_channels*self.factor, n=self.gh*self.gw)  #number of channels????????????????\n        self.out_project = nn.Linear(self.num_channels*self.factor//2, self.num_channels, self.use_bias)   #c*factor->c\n        self.dropout = nn.Dropout(self.dropout_rate)\n    def forward(self, x):\n        n, h, w, num_channels = x.shape\n        fh, fw = h // self.gh, w // self.gw\n        x = block_images_einops(x, patch_size=(fh, fw))  #n (gh gw) (fh fw) c\n        # gMLP1: Global (grid) mixing part, provides global grid communication.\n        y = self.LayerNorm(x)\n        y = self.in_project(y)  #channel proj\n        y = self.gelu(y)\n        y = self.GridGatingUnit(y)\n        y = self.out_project(y)\n        y = self.dropout(y)\n        x = x + y\n        x = unblock_images_einops(x, grid_size=(self.gh, self.gw), patch_size=(fh, fw))\n        return x\n\nclass BlockGmlpLayer(nn.Module):  #input shape: n, h, w, c\n    \"\"\"Block gMLP layer that performs local mixing of tokens.\"\"\"\n    def __init__(self, block_size, num_channels, use_bias=True, factor=2, dropout_rate=0):\n        super().__init__()\n        self.block_size = block_size\n        self.fh = self.block_size[0]\n        self.fw = self.block_size[1]\n        self.num_channels = num_channels\n        self.use_bias = use_bias\n        self.factor = factor\n        self.dropout_rate = dropout_rate\n        self.LayerNorm = Layer_norm_process(self.num_channels)\n        self.in_project = nn.Linear(self.num_channels, self.num_channels*self.factor, self.use_bias)   #c->c*factor\n        self.gelu = nn.GELU(approximate='tanh')\n        self.BlockGatingUnit = BlockGatingUnit(self.num_channels*self.factor, n=self.fh*self.fw)  #number of channels????????????????\n        self.out_project = nn.Linear(self.num_channels*self.factor//2, self.num_channels, self.use_bias)   #c*factor->c\n        self.dropout = nn.Dropout(self.dropout_rate)\n    def forward(self, x):\n        _, h, w, _ = x.shape\n        gh, gw = h // self.fh, w // self.fw\n        x = block_images_einops(x, patch_size=(self.fh, self.fw))  #n (gh gw) (fh fw) c\n        # gMLP2: Local (block) mixing part, provides local block communication.\n        y = self.LayerNorm(x)\n        y = self.in_project(y)  #channel proj\n        y = self.gelu(y)\n        y = self.BlockGatingUnit(y)\n        y = self.out_project(y)\n        y = self.dropout(y)\n        x = x + y\n        x = unblock_images_einops(x, grid_size=(gh, gw), patch_size=(self.fh, self.fw))\n        return x\n\nclass MlpBlock(nn.Module):  #input shape: n, h, w, c\n    \"\"\"A 1-hidden-layer MLP block, applied over the last dimension.\"\"\"\n    def __init__(self, mlp_dim , dropout_rate=0.,use_bias=True):\n        super().__init__()\n        self.mlp_dim=mlp_dim\n        self.dropout_rate=dropout_rate\n        self.use_bias=use_bias\n        self.Dense_0 = nn.Linear(self.mlp_dim, self.mlp_dim,bias=self.use_bias)\n        self.gelu = nn.GELU(approximate='tanh')\n        self.dropout = nn.Dropout(self.dropout_rate)\n        self.Dense_1 = nn.Linear(self.mlp_dim, self.mlp_dim,bias=self.use_bias)\n\n    def forward(self, x):\n        x = self.Dense_0(x)\n        x = self.gelu(x)\n        x = self.dropout(x)\n        x = self.Dense_1(x)\n        return x\n\nclass CALayer(nn.Module):  #input shape: n, h, w, c\n    \"\"\"Squeeze-and-excitation block for channel attention.\n    ref: https://arxiv.org/abs/1709.01507\n    \"\"\"\n    def __init__(self, features, reduction=4, use_bias=True):\n        super().__init__()\n        self.features = features\n        self.reduction = reduction\n        self.use_bias = use_bias\n        self.Conv_0 = nn.Conv2d(self.features, self.features//self.reduction, kernel_size=(1,1), stride=1, bias=self.use_bias)  #1*1 conv\n        self.relu = nn.ReLU()\n        self.Conv_1 = nn.Conv2d(self.features//self.reduction, self.features, kernel_size=(1,1), stride=1, bias=self.use_bias)  #1*1 conv\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):\n        y = x.permute(0,3,1,2)  #n, c, h, w\n        y = torch.mean(y, dim=(2,3), keepdim=True)  #keep dimensions for element product in the last step\n        y = self.Conv_0(y)\n        y = self.relu(y)\n        y = self.Conv_1(y)\n        y = self.sigmoid(y)\n        y = y.permute(0,2,3,1)  #n, h, w, c\n        return x*y\n\nclass GetSpatialGatingWeights(nn.Module):  #n, h, w, c\n    \"\"\"Get gating weights for cross-gating MLP block.\"\"\"\n    def __init__(self, num_channels, grid_size, block_size, input_proj_factor=2, use_bias=True, dropout_rate=0):\n        super().__init__()\n        self.num_channels = num_channels\n        self.grid_size = grid_size\n        self.block_size = block_size\n        self.gh = self.grid_size[0]\n        self.gw = self.grid_size[1]\n        self.fh = self.block_size[0]\n        self.fw = self.block_size[1]\n        self.input_proj_factor = input_proj_factor\n        self.use_bias = use_bias\n        self.drop = dropout_rate\n        self.LayerNorm_in = Layer_norm_process(self.num_channels)\n        self.in_project = nn.Linear(self.num_channels, self.num_channels*self.input_proj_factor, bias=self.use_bias)\n        self.gelu = nn.GELU(approximate='tanh')\n        self.Dense_0 = nn.Linear(self.gh*self.gw, self.gh*self.gw, bias = self.use_bias)\n        self.Dense_1 = nn.Linear(self.fh*self.fw, self.fh*self.fw, bias = self.use_bias)\n        self.out_project = nn.Linear(self.num_channels*self.input_proj_factor, self.num_channels, bias=self.use_bias)\n        self.dropout = nn.Dropout(self.drop)\n    def forward(self, x):\n        _, h, w, _ = x.shape\n        #input projection\n        x = self.LayerNorm_in(x)\n        x = self.in_project(x)  #channel projection\n        x = self.gelu(x)\n        c = x.size(-1)//2\n        u, v = torch.split(x, c, dim=-1)\n        #get grid MLP weights\n        fh, fw = h//self.gh, w//self.gw\n        u = block_images_einops(u, patch_size = (fh, fw))   #n, (gh gw) (fh fw) c\n        u = u.permute(0,3,2,1)  #n, c, (fh fw) (gh gw)\n        u = self.Dense_0(u)\n        u = u.permute(0,3,2,1)  #n, (gh gw) (fh fw) c\n        u = unblock_images_einops(u, grid_size=(self.gh, self.gw), patch_size=(fh, fw))\n        #get block MLP weights\n        gh, gw = h//self.fh, w//self.fw\n        v = block_images_einops(v, patch_size=(self.fh, self.fw))   #n, (gh gw) (fh fw) c\n        v = v.permute(0,1,3,2)  #n (gh gw) c (fh fw)\n        v = self.Dense_1(v)\n        v = v.permute(0,1,3,2)  #n, (gh gw) (fh fw) c\n        v = unblock_images_einops(v, grid_size=(gh, gw), patch_size=(self.fh, self.fw))\n        \n        x = torch.cat([u,v], dim=-1)\n        x = self.out_project(x)\n        x = self.dropout(x)\n        return x\n\nclass ResidualSplitHeadMultiAxisGmlpLayer(nn.Module):   #input shape: n, h, w, c\n    \"\"\"The multi-axis gated MLP block.\"\"\"\n    def __init__(self, block_size, grid_size, num_channels, input_proj_factor=2,block_gmlp_factor=2,grid_gmlp_factor=2,use_bias=True,dropout_rate=0.):\n        super().__init__()\n        self.block_size = block_size\n        self.grid_size = grid_size\n        self.num_channels = num_channels\n        self.input_proj_factor = input_proj_factor\n        self.block_gmlp_factor = block_gmlp_factor\n        self.grid_gmlp_factor = grid_gmlp_factor\n        self.use_bias = use_bias\n        self.drop = dropout_rate\n        self.LayerNorm_in = Layer_norm_process(self.num_channels)\n        self.in_project = nn.Linear(self.num_channels, self.num_channels*self.input_proj_factor, bias=self.use_bias)\n        self.gelu = nn.GELU(approximate='tanh')\n        self.GridGmlpLayer = GridGmlpLayer(grid_size=self.grid_size, num_channels=self.num_channels*self.input_proj_factor//2, \n                                            use_bias=self.use_bias, factor=self.grid_gmlp_factor)\n        self.BlockGmlpLayer = BlockGmlpLayer(block_size=self.block_size, num_channels=self.num_channels*self.input_proj_factor//2, \n                                             use_bias=self.use_bias, factor=self.block_gmlp_factor)\n        self.out_project = nn.Linear(self.num_channels*self.input_proj_factor, self.num_channels, bias=self.use_bias)\n        self.dropout = nn.Dropout(self.drop)\n    def forward(self, x):\n        shortcut = x\n        x = self.LayerNorm_in(x)\n        x = self.in_project(x)\n        x = self.gelu(x)\n        c = x.size(-1)//2\n        u, v = torch.split(x, c, dim=-1)\n        #grid gMLP\n        u = self.GridGmlpLayer(u)\n        #block gMLP\n        v = self.BlockGmlpLayer(v)\n        #out projection\n        x = torch.cat([u,v], dim=-1)\n        x = self.out_project(x)\n        x = self.dropout(x)\n        x = x + shortcut\n        return x\n\nclass RCAB(nn.Module):  #input shape: n, h, w, c\n    \"\"\"Residual channel attention block. Contains LN,Conv,lRelu,Conv,SELayer.\"\"\"\n    def __init__(self,features, reduction=4, lrelu_slope=0.2, use_bias=True):\n        super().__init__()\n        self.features = features\n        self.reduction = reduction\n        self.lrelu_slope = lrelu_slope\n        self.bias = use_bias\n        self.LayerNorm = Layer_norm_process(self.features)\n        self.conv1 = nn.Conv2d(self.features, self.features, kernel_size=(3,3),stride=1,bias=self.bias,padding=1)\n        self.leaky_relu = nn.LeakyReLU(negative_slope=self.lrelu_slope)\n        self.conv2 = nn.Conv2d(self.features, self.features, kernel_size=(3,3),stride=1,bias=self.bias,padding=1)\n        self.channel_attention = CALayer(features=self.features, reduction=self.reduction)\n    def forward(self,x):\n        shortcut = x\n        x = self.LayerNorm(x)\n        x = x.permute(0,3,1,2)  #n, c, h, w\n        x = self.conv1(x)\n        x = self.leaky_relu(x)\n        x = self.conv2(x)\n        x = x.permute(0,2,3,1)  #n, h, w, c\n        x = self.channel_attention(x)\n        return x + shortcut\n\nclass RDCAB(nn.Module):  #input shape: n, h, w, c\n    \"\"\"Residual dense channel attention block. Used in Bottlenecks.\"\"\"\n    def __init__(self,features, reduction=4, dropout_rate=0, use_bias=True):\n        super().__init__()\n        self.features = features\n        self.reduction = reduction\n        self.drop = dropout_rate\n        self.bias = use_bias\n        self.LayerNorm = Layer_norm_process(self.features)\n        self.channel_mixing = MlpBlock(mlp_dim=self.features, dropout_rate=self.drop,use_bias=self.bias)\n        self.channel_attention = CALayer(features=self.features, reduction=self.reduction, use_bias=self.bias)\n    def forward(self,x):\n        y = self.LayerNorm(x)\n        y = self.channel_mixing(y)\n        y = self.channel_attention(y)\n        x = x + y\n        return x\n\nclass CrossGatingBlock(nn.Module):  #input shape: n, c, h, w\n    \"\"\"Cross-gating MLP block.\"\"\"\n    def __init__(self, x_features, num_channels, block_size, grid_size, cin_y=0,upsample_y=True, use_bias=True, use_global_mlp=True, dropout_rate=0):\n        super().__init__()\n        self.cin_y = cin_y\n        self.x_features = x_features\n        self.num_channels = num_channels\n        self.block_size = block_size\n        self.grid_size = grid_size\n        self.upsample_y = upsample_y\n        self.use_bias = use_bias\n        self.use_global_mlp = use_global_mlp\n        self.drop = dropout_rate\n        self.ConvTranspose_0 = nn.ConvTranspose2d(self.cin_y,self.num_channels,kernel_size=(2,2),stride=2,bias=self.use_bias)\n        self.Conv_0 = nn.Conv2d(self.x_features, self.num_channels, kernel_size=(1,1),stride=1, bias=self.use_bias)\n        self.Conv_1 = nn.Conv2d(self.num_channels, self.num_channels, kernel_size=(1,1),stride=1, bias=self.use_bias)\n        self.LayerNorm_x = Layer_norm_process(self.num_channels)\n        self.in_project_x = nn.Linear(self.num_channels, self.num_channels, bias=self.use_bias)\n        self.gelu1 = nn.GELU(approximate='tanh')\n        self.SplitHeadMultiAxisGating_x = GetSpatialGatingWeights(num_channels=self.num_channels,block_size=self.block_size,grid_size=self.grid_size,\n            dropout_rate=self.drop,use_bias=self.use_bias)\n        self.LayerNorm_y = Layer_norm_process(self.num_channels)\n        self.in_project_y = nn.Linear(self.num_channels, self.num_channels, bias=self.use_bias)\n        self.gelu2 = nn.GELU(approximate='tanh')\n        self.SplitHeadMultiAxisGating_y = GetSpatialGatingWeights(num_channels=self.num_channels,block_size=self.block_size,grid_size=self.grid_size,\n            dropout_rate=self.drop,use_bias=self.use_bias)\n        self.out_project_y = nn.Linear(self.num_channels, self.num_channels, bias=self.use_bias)\n        self.dropout1 = nn.Dropout(self.drop)\n        self.out_project_x = nn.Linear(self.num_channels, self.num_channels, bias=self.use_bias)\n        self.dropout2 = nn.Dropout(self.drop)\n    def forward(self, x,y):\n        # Upscale Y signal, y is the gating signal.\n        if self.upsample_y:\n                y = self.ConvTranspose_0(y)\n        x = self.Conv_0(x)\n        y = self.Conv_1(y)\n        assert y.shape == x.shape\n        x = x.permute(0,2,3,1)  #n,h,w,c\n        y = y.permute(0,2,3,1)  #n,h,w,c\n        shortcut_x = x\n        shortcut_y = y\n        # Get gating weights from X\n        x = self.LayerNorm_x(x)\n        x = self.in_project_x(x)\n        x = self.gelu1(x)\n        gx = self.SplitHeadMultiAxisGating_x(x)\n        # Get gating weights from Y\n        y = self.LayerNorm_y(y)\n        y = self.in_project_y(y)\n        y = self.gelu2(y)\n        gy = self.SplitHeadMultiAxisGating_y(y)\n        # Apply cross gating\n        y = y * gx  ## gating y using x\n        y = self.out_project_y(y)\n        y = self.dropout1(y)\n        y = y + shortcut_y\n        x = x * gy  # gating x using y\n        x = self.out_project_x(x)\n        x = self.dropout2(x)\n        x = x + y + shortcut_x  # get all aggregated signals\n        return x.permute(0,3,1,2), y.permute(0,3,1,2)  #n,c,h,w\n\nclass UNetEncoderBlock(nn.Module):  #input shape: n, c, h, w (pytorch default)\n    \"\"\"Encoder block in MAXIM.\"\"\"\n    def __init__(self, cin, num_channels, block_size, grid_size, dec=False, lrelu_slope=0.2,block_gmlp_factor=2, grid_gmlp_factor=2,\n                input_proj_factor=2, channels_reduction=4, dropout_rate=0., use_bias=True,downsample=True,use_global_mlp=True):\n        super().__init__()\n        self.cin = cin\n        self.num_channels = num_channels\n        self.block_size = block_size\n        self.grid_size = grid_size\n        self.lrelu_slope = lrelu_slope\n        self.block_gmlp_factor = block_gmlp_factor\n        self.grid_gmlp_factor = grid_gmlp_factor\n        self.input_proj_factor = input_proj_factor\n        self.reduction = channels_reduction\n        self.drop = dropout_rate\n        self.dec = dec\n        self.use_bias = use_bias\n        self.downsample = downsample\n        self.use_global_mlp = use_global_mlp\n        self.Conv_0 = nn.Conv2d(self.cin,self.num_channels,kernel_size=(1,1),stride=(1,1),bias=self.use_bias)\n        self.SplitHeadMultiAxisGmlpLayer_0 = ResidualSplitHeadMultiAxisGmlpLayer(block_size=self.block_size, \n                            grid_size=self.grid_size, num_channels=self.num_channels, input_proj_factor=self.input_proj_factor,\n                            block_gmlp_factor=self.block_gmlp_factor, grid_gmlp_factor=self.grid_gmlp_factor, dropout_rate=self.drop, use_bias=self.use_bias)\n        self.SplitHeadMultiAxisGmlpLayer_1 = ResidualSplitHeadMultiAxisGmlpLayer(block_size=self.block_size, \n                            grid_size=self.grid_size, num_channels=self.num_channels, input_proj_factor=self.input_proj_factor,\n                            block_gmlp_factor=self.block_gmlp_factor, grid_gmlp_factor=self.grid_gmlp_factor, dropout_rate=self.drop, use_bias=self.use_bias)\n        self.channel_attention_block_10 = RCAB(features=self.num_channels, reduction=self.reduction, lrelu_slope=self.lrelu_slope, use_bias=self.use_bias)\n        self.channel_attention_block_11 = RCAB(features=self.num_channels, reduction=self.reduction, lrelu_slope=self.lrelu_slope, use_bias=self.use_bias)\n        self.cross_gating_block = CrossGatingBlock(x_features=self.num_channels, num_channels=self.num_channels, block_size=self.block_size, \n                            grid_size=self.grid_size, upsample_y=False, dropout_rate=self.drop, use_bias=self.use_bias, use_global_mlp=self.use_global_mlp)\n        self.Conv_1 = nn.Conv2d(self.num_channels,self.num_channels,kernel_size=(4,4),stride=2,padding=1)\n    def forward(self, x,skip=None,enc=None,dec=None):\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n        x = self.Conv_0(x)\n        shortcut_long = x\n        x = x.permute(0,2,3,1)  #n,h,w,c\n        if self.use_global_mlp:\n            x = self.SplitHeadMultiAxisGmlpLayer_0(x)\n        x = self.channel_attention_block_10(x)\n        if self.use_global_mlp:\n            x = self.SplitHeadMultiAxisGmlpLayer_1(x)\n        x = self.channel_attention_block_11(x)\n        x = x.permute(0,3,1,2)  #n,c,h,w\n        x = x + shortcut_long\n        if enc is not None and dec is not None:  #if stage>0\n            x, _ = self.cross_gating_block(x,enc+dec)\n        if self.downsample:\n            x_down = self.Conv_1(x)\n            return x_down, x\n        else:\n            return x\n\nclass UNetDecoderBlock(nn.Module):  #input shape: n, c, h, w\n    \"\"\"Decoder block in MAXIM.\"\"\"\n    def __init__(self, cin, num_channels, block_size, grid_size, lrelu_slope=0.2, block_gmlp_factor=2, grid_gmlp_factor=2,\n                 input_proj_factor=2, channels_reduction=4, dropout_rate=0., use_bias=True, downsample=True, use_global_mlp=True):\n        super().__init__()\n        self.cin = cin\n        self.num_channels = num_channels\n        self.block_size = block_size\n        self.grid_size = grid_size\n        self.lrelu_slope = lrelu_slope\n        self.block_gmlp_factor = block_gmlp_factor\n        self.grid_gmlp_factor = grid_gmlp_factor\n        self.input_proj_factor = input_proj_factor\n        self.reduction = channels_reduction\n        self.dropout_rate = dropout_rate\n        self.use_bias = use_bias\n        self.downsample = downsample\n        self.use_global_mlp = use_global_mlp\n        self.ConvTranspose_0 = nn.ConvTranspose2d(self.cin,self.num_channels,kernel_size=(2,2),stride=2,bias=self.use_bias)\n        self.UNetEncoderBlock_0 = UNetEncoderBlock(4*self.num_channels, self.num_channels, self.block_size, self.grid_size, lrelu_slope=self.lrelu_slope,\n                block_gmlp_factor=self.block_gmlp_factor, grid_gmlp_factor=self.grid_gmlp_factor, dec = True,\n                input_proj_factor=self.input_proj_factor, channels_reduction=self.reduction, dropout_rate=self.dropout_rate, use_bias=self.use_bias,downsample=False,use_global_mlp=self.use_global_mlp)\n    def forward(self, x, bridge=None):\n        x = self.ConvTranspose_0(x)\n        x = self.UNetEncoderBlock_0(x,skip=bridge)\n        return x\n\nclass BottleneckBlock(nn.Module):  #input shape: n,c,h,w\n    \"\"\"The bottleneck block consisting of multi-axis gMLP block and RDCAB.\"\"\"\n    def __init__(self,features, block_size, grid_size,block_gmlp_factor=2,grid_gmlp_factor=2,input_proj_factor=2,channels_reduction=4,use_bias=True, dropout_rate=0.):\n        super().__init__()\n        self.features = features\n        self.block_size = block_size\n        self.grid_size = grid_size\n        self.block_gmlp_factor = block_gmlp_factor\n        self.grid_gmlp_factor = grid_gmlp_factor\n        self.input_proj_factor = input_proj_factor\n        self.channels_reduction = channels_reduction\n        self.use_bias = use_bias\n        self.drop = dropout_rate\n        self.input_proj = nn.Conv2d(self.features,self.features,kernel_size=(1,1),stride=1)\n        self.SplitHeadMultiAxisGmlpLayer_0 = ResidualSplitHeadMultiAxisGmlpLayer(block_size=self.block_size, grid_size=self.grid_size, num_channels=self.features, \n                                    input_proj_factor=self.input_proj_factor,block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,use_bias=self.use_bias)\n        self.SplitHeadMultiAxisGmlpLayer_1 = ResidualSplitHeadMultiAxisGmlpLayer(block_size=self.block_size, grid_size=self.grid_size, num_channels=self.features, \n                                    input_proj_factor=self.input_proj_factor,block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,use_bias=self.use_bias)\n        self.channel_attention_block_1_0 = RDCAB(features, dropout_rate=self.drop, use_bias=self.use_bias)\n        self.channel_attention_block_1_1 = RDCAB(features, dropout_rate=self.drop, use_bias=self.use_bias)\n\n    def forward(self, x):\n        assert x.ndim == 4  # Input has shape [batch, c, h, w]\n        # input projection\n        x = self.input_proj(x)\n        shortcut_long = x\n        x = x.permute(0,2,3,1)  #n, h, w, c\n        x = self.SplitHeadMultiAxisGmlpLayer_0(x)\n        x = self.channel_attention_block_1_0(x)\n        x = self.SplitHeadMultiAxisGmlpLayer_1(x)\n        x = self.channel_attention_block_1_1(x)\n        x = x.permute(0,3,1,2)  #n, c, h, w\n        x = x + shortcut_long\n        return x\n\n#multi stage\nclass SAM(nn.Module):  #x shape and x_image shape: n, c, h, w\n    \"\"\"Supervised attention module for multi-stage training.\n    Introduced by MPRNet [CVPR2021]: https://github.com/swz30/MPRNet\n    \"\"\"\n    def __init__(self,features,output_channels=3,use_bias=True):\n        super().__init__()\n        self.features = features  #cin\n        self.output_channels = output_channels\n        self.use_bias = use_bias\n        self.Conv_0 = nn.Conv2d(self.features,self.features, kernel_size=(3, 3),bias=self.use_bias,padding=1)\n        self.Conv_1 = nn.Conv2d(self.features,self.output_channels, kernel_size=(3, 3),bias=self.use_bias,padding=1)\n        self.Conv_2 = nn.Conv2d(self.output_channels,self.features, kernel_size=(3, 3),bias=self.use_bias,padding=1)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self,x,x_image):\n        \"\"\"Apply the SAM module to the input and features.\n        Args:\n          x: the output features from UNet decoder with shape (h, w, c)\n          x_image: the input image with shape (h, w, 3)\n          train: Whether it is training\n        Returns:\n          A tuple of tensors (x1, image) where (x1) is the sam features used for the\n            next stage, and (image) is the output restored image at current stage.\n        \"\"\"\n        # Get features\n        x1 = self.Conv_0(x)\n        # Output restored image X_s\n        if self.output_channels == 3:\n            image = self.Conv_1(x) + x_image\n        else:\n            image = self.Conv_1(x)\n        # Get attention maps for features\n        x2 = self.Conv_2(image)\n        x2 = self.sigmoid(x2)\n        # Get attended feature maps\n        x1 = x1 * x2\n        # Residual connection\n        x1 = x1 + x\n        return x1, image\n\n#top: 3-stage MAXIM for image denoising\nclass MAXIM_dns_3s(nn.Module):  #input shape: n, c, h, w   \n    \"\"\"The MAXIM model function with multi-stage and multi-scale supervision.\n    For more model details, please check the CVPR paper:\n    MAXIM: MUlti-Axis MLP for Image Processing (https://arxiv.org/abs/2201.02973)\n    Attributes:\n      features: initial hidden dimension for the input resolution.\n      depth: the number of downsampling depth for the model.\n      num_stages: how many stages to use. It will also affects the output list.\n      use_bias: whether to use bias in all the conv/mlp layers.\n      num_supervision_scales: the number of desired supervision scales.\n      lrelu_slope: the negative slope parameter in leaky_relu layers.\n      use_global_mlp: whether to use the multi-axis gated MLP block (MAB) in each\n        layer.\n      use_cross_gating: whether to use the cross-gating MLP block (CGB) in the\n        skip connections and multi-stage feature fusion layers.\n      high_res_stages: how many stages are specificied as high-res stages. The\n        rest (depth - high_res_stages) are called low_res_stages.\n      block_size_hr: the block_size parameter for high-res stages.\n      block_size_lr: the block_size parameter for low-res stages.\n      grid_size_hr: the grid_size parameter for high-res stages.\n      grid_size_lr: the grid_size parameter for low-res stages.\n      block_gmlp_factor: the input projection factor for block_gMLP layers.\n      grid_gmlp_factor: the input projection factor for grid_gMLP layers.\n      input_proj_factor: the input projection factor for the MAB block.\n      channels_reduction: the channel reduction factor for SE layer.\n      num_outputs: the output channels.\n      dropout_rate: Dropout rate.\n    Returns:\n      The output contains a list of arrays consisting of multi-stage multi-scale\n      outputs. For example, if num_stages = num_supervision_scales = 3 (the\n      model used in the paper), the output specs are: outputs =\n      [[output_stage1_scale1, output_stage1_scale2, output_stage1_scale3],\n       [output_stage2_scale1, output_stage2_scale2, output_stage2_scale3],\n       [output_stage3_scale1, output_stage3_scale2, output_stage3_scale3],]\n      The final output can be retrieved by outputs[-1][-1].\n    \"\"\"\n    def __init__(self,features=32,depth=3, use_bias=True, num_supervision_scales=int(3), lrelu_slope=0.2,\n                 use_global_mlp=True,high_res_stages=2,block_size_hr=(16,16),block_size_lr=(8,8),\n                 grid_size_hr=(16, 16),grid_size_lr=(8, 8),\n                block_gmlp_factor=2, grid_gmlp_factor=2,input_proj_factor=2, channels_reduction=4, num_outputs=3, dropout_rate=0.):\n        super().__init__()\n        self.features = features\n        self.depth = depth\n        self.bias = use_bias\n        self.num_supervision_scales = num_supervision_scales\n        self.lrelu_slope = lrelu_slope\n        self.use_global_mlp = use_global_mlp\n        self.high_res_stages = high_res_stages\n        self.block_size_hr = block_size_hr\n        self.block_size_lr = block_size_lr\n        self.grid_size_hr = grid_size_hr\n        self.grid_size_lr = grid_size_lr\n        self.block_gmlp_factor = block_gmlp_factor\n        self.grid_gmlp_factor = grid_gmlp_factor\n        self.input_proj_factor = input_proj_factor\n        self.channels_reduction = channels_reduction\n        self.num_outputs = num_outputs\n        self.drop = dropout_rate\n\n        ########## STAGE 0 ##########\n        #multi scale input and encoder \n        #depth=0\n        self.stage_0_input_conv_0 = nn.Conv2d(3,self.features,kernel_size=(3,3),bias=self.bias,padding=1)\n        self.stage_0_encoder_block_0 = UNetEncoderBlock(cin=2*self.features, num_channels=self.features, \n                block_size=self.block_size_hr if 0 < self.high_res_stages else self.block_size_lr, \n                grid_size=self.grid_size_hr if 0 < self.high_res_stages else self.block_size_lr, \n                lrelu_slope=self.lrelu_slope, block_gmlp_factor=self.block_gmlp_factor, grid_gmlp_factor=self.grid_gmlp_factor,\n                input_proj_factor=self.input_proj_factor, channels_reduction=self.channels_reduction, dropout_rate=self.drop, use_bias=self.bias,\n                downsample=True,use_global_mlp=self.use_global_mlp)\n        #depth=1\n        self.stage_0_input_conv_1 = nn.Conv2d(3,2*self.features,kernel_size=(3,3),bias=self.bias,padding=1)\n        self.stage_0_encoder_block_1 = UNetEncoderBlock(cin = 3*self.features, num_channels=2*self.features, \n                block_size=self.block_size_hr if 1 < self.high_res_stages else self.block_size_lr, \n                grid_size=self.grid_size_hr if 1 < self.high_res_stages else self.block_size_lr, \n                lrelu_slope=self.lrelu_slope, block_gmlp_factor=self.block_gmlp_factor, grid_gmlp_factor=self.grid_gmlp_factor,\n                input_proj_factor=self.input_proj_factor, channels_reduction=self.channels_reduction, dropout_rate=self.drop, use_bias=self.bias,\n                downsample=True,use_global_mlp=self.use_global_mlp)\n        #depth=2\n        self.stage_0_input_conv_2 = nn.Conv2d(3,4*self.features,kernel_size=(3,3),bias=self.bias,padding=1)\n        self.stage_0_encoder_block_2 = UNetEncoderBlock(cin=6*self.features, num_channels=4*self.features, \n                block_size=self.block_size_hr if 2 < self.high_res_stages else self.block_size_lr, \n                grid_size=self.grid_size_hr if 2 < self.high_res_stages else self.block_size_lr, \n                lrelu_slope=self.lrelu_slope, block_gmlp_factor=self.block_gmlp_factor, grid_gmlp_factor=self.grid_gmlp_factor,\n                input_proj_factor=self.input_proj_factor, channels_reduction=self.channels_reduction, dropout_rate=self.drop, use_bias=self.bias,\n                downsample=True,use_global_mlp=self.use_global_mlp)\n\n        #bottleneck\n        self.stage_0_global_block_0 = BottleneckBlock(block_size=self.block_size_lr,grid_size=self.grid_size_lr,features=4 * self.features,\n            block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,input_proj_factor=self.input_proj_factor,\n            dropout_rate=self.drop,use_bias=self.bias,channels_reduction=self.channels_reduction)\n        self.stage_0_global_block_1 = BottleneckBlock(block_size=self.block_size_lr,grid_size=self.grid_size_lr,features=4 * self.features,\n            block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,input_proj_factor=self.input_proj_factor,\n            dropout_rate=self.drop,use_bias=self.bias,channels_reduction=self.channels_reduction)\n\n        #cross gating (within a stage)\n        #depth=2\n        self.UpSampleRatio_0 = UpSampleRatio_4(1*self.features,ratio=2**(-2),use_bias=self.bias)  #0->2\n        self.UpSampleRatio_1 = UpSampleRatio_2(2 * self.features,ratio=2**(-1),use_bias=self.bias)#1->2\n        self.UpSampleRatio_2 = UpSampleRatio(4 * self.features,ratio=1,use_bias=self.bias)#2->2\n        self.stage_0_cross_gating_block_2 = CrossGatingBlock(cin_y=4*features,x_features=3*(2**2) * self.features, num_channels=4*features,\n              block_size=self.block_size_hr if 2 < self.high_res_stages else self.block_size_lr,\n              grid_size=self.grid_size_hr if 2 < self.high_res_stages else self.block_size_lr,\n              upsample_y=True,use_bias=self.bias,dropout_rate=self.drop)\n        #depth=1\n        self.UpSampleRatio_3 = UpSampleRatio_2(1 * self.features,ratio=2**(-1),use_bias=self.bias)#0->1\n        self.UpSampleRatio_4 = UpSampleRatio(2 * self.features,ratio=2**(0),use_bias=self.bias)#1->1\n        self.UpSampleRatio_5 = UpSampleRatio_1_2(4 * self.features,ratio=2,use_bias=self.bias)#2->1\n        self.stage_0_cross_gating_block_1 = CrossGatingBlock(cin_y=4*features,x_features=3*2 * self.features,num_channels=2*features,\n              block_size=self.block_size_hr if 1 < self.high_res_stages else self.block_size_lr,\n              grid_size=self.grid_size_hr if 1 < self.high_res_stages else self.block_size_lr,\n              upsample_y=True,use_bias=self.bias,dropout_rate=self.drop)\n        #depth=0\n        self.UpSampleRatio_6 = UpSampleRatio(1 * self.features,ratio=1,use_bias=self.bias)#0->0\n        self.UpSampleRatio_7 = UpSampleRatio_1_2(2 * self.features,ratio=2,use_bias=self.bias)#1->0\n        self.UpSampleRatio_8 = UpSampleRatio_1_4(4 * self.features,ratio=4,use_bias=self.bias)#2->0\n        self.stage_0_cross_gating_block_0 = CrossGatingBlock(cin_y=2*features,x_features=3 * self.features,num_channels=self.features,\n              block_size=self.block_size_hr if 0 < self.high_res_stages else self.block_size_lr,\n              grid_size=self.grid_size_hr if 0 < self.high_res_stages else self.block_size_lr,\n              upsample_y=True,use_bias=self.bias,dropout_rate=self.drop)\n\n        #decoder\n        #depth=2\n        self.UpSampleRatio_9 = UpSampleRatio(4 * self.features,ratio=2**(0),use_bias=self.bias)#2->2\n        self.UpSampleRatio_10 = UpSampleRatio_2(2 * self.features,ratio=2**(-1),use_bias=self.bias)#1->2\n        self.UpSampleRatio_11 = UpSampleRatio_4(1 * self.features,ratio=2**(-2),use_bias=self.bias)#0->2\n        self.stage_0_decoder_block_2 = UNetDecoderBlock(cin=4*self.features, num_channels=(2**2)*self.features,lrelu_slope=self.lrelu_slope,\n            block_size=self.block_size_hr if 2 < self.high_res_stages else self.block_size_lr,\n            grid_size=self.grid_size_hr if 2 < self.high_res_stages else self.block_size_lr,\n            block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,input_proj_factor=self.input_proj_factor,\n            channels_reduction=self.channels_reduction,use_global_mlp=self.use_global_mlp,dropout_rate=self.drop,use_bias=self.bias)\n        self.stage_0_supervised_attention_module_2 = SAM(features=2 ** (2)*self.features,output_channels=self.num_outputs,use_bias=self.bias)\n        #depth=1\n        self.UpSampleRatio_12 = UpSampleRatio_1_2(4 * self.features,ratio=2**(1),use_bias=self.bias)#2->1\n        self.UpSampleRatio_13 = UpSampleRatio(2 * self.features,ratio=2**(0),use_bias=self.bias)#1->1\n        self.UpSampleRatio_14 = UpSampleRatio_2(1 * self.features,ratio=2**(-1),use_bias=self.bias)#0->1\n        self.stage_0_decoder_block_1 = UNetDecoderBlock(cin=4*self.features, num_channels=(2**1)*self.features,lrelu_slope=self.lrelu_slope,\n            block_size=self.block_size_hr if 1 < self.high_res_stages else self.block_size_lr,\n            grid_size=self.grid_size_hr if 1 < self.high_res_stages else self.block_size_lr,\n            block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,input_proj_factor=self.input_proj_factor,\n            channels_reduction=self.channels_reduction,use_global_mlp=self.use_global_mlp,dropout_rate=self.drop,use_bias=self.bias)\n        self.stage_0_supervised_attention_module_1 = SAM(features=2 ** (1)*self.features,output_channels=self.num_outputs,use_bias=self.bias)\n        #depth=0\n        self.UpSampleRatio_15 = UpSampleRatio_1_4(4 * self.features,ratio=4,use_bias=self.bias)#2->0\n        self.UpSampleRatio_16 = UpSampleRatio_1_2(2 * self.features,ratio=2,use_bias=self.bias)#1->0\n        self.UpSampleRatio_17 = UpSampleRatio(1 * self.features,ratio=1,use_bias=self.bias)#0->0\n        self.stage_0_decoder_block_0 = UNetDecoderBlock(cin=2*self.features,num_channels=self.features,lrelu_slope=self.lrelu_slope,\n            block_size=self.block_size_hr if 0 < self.high_res_stages else self.block_size_lr,\n            grid_size=self.grid_size_hr if 0 < self.high_res_stages else self.block_size_lr,\n            block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,input_proj_factor=self.input_proj_factor,\n            channels_reduction=self.channels_reduction,use_global_mlp=self.use_global_mlp,dropout_rate=self.drop,use_bias=self.bias)\n        self.stage_0_supervised_attention_module_0 = SAM(features=2 ** (0)*self.features,output_channels=self.num_outputs,use_bias=self.bias)\n\n        ########## STAGE 1 ##########\n        #multi scale input and encoder \n        #depth=0\n        self.stage_1_input_conv_0 = nn.Conv2d(3,self.features,kernel_size=(3,3),bias=self.bias,padding=1)\n        self.stage_1_input_fuse_sam_0 = CrossGatingBlock(x_features=self.features,num_channels=self.features,\n              block_size=self.block_size_hr if 0 < self.high_res_stages else self.block_size_lr,\n              grid_size=self.grid_size_hr if 0 < self.high_res_stages else self.block_size_lr,\n              upsample_y=False,use_bias=self.bias,dropout_rate=self.drop)\n        self.stage_1_input_catconv_0 = nn.Conv2d(2*self.features,self.features,kernel_size=(1,1),bias=self.bias)\n        self.stage_1_encoder_block_0 = UNetEncoderBlock(cin=2*self.features, num_channels=self.features, \n                block_size=self.block_size_hr if 0 < self.high_res_stages else self.block_size_lr, \n                grid_size=self.grid_size_hr if 0 < self.high_res_stages else self.block_size_lr, \n                lrelu_slope=self.lrelu_slope, block_gmlp_factor=self.block_gmlp_factor, grid_gmlp_factor=self.grid_gmlp_factor,\n                input_proj_factor=self.input_proj_factor, channels_reduction=self.channels_reduction, dropout_rate=self.drop, use_bias=self.bias,\n                downsample=True,use_global_mlp=self.use_global_mlp)\n        #depth=1\n        self.stage_1_input_conv_1 = nn.Conv2d(3,2*self.features,kernel_size=(3,3),bias=self.bias,padding=1)\n        self.stage_1_input_fuse_sam_1 = CrossGatingBlock(x_features=2*self.features,num_channels=2*self.features,\n              block_size=self.block_size_hr if 1 < self.high_res_stages else self.block_size_lr,\n              grid_size=self.grid_size_hr if 1 < self.high_res_stages else self.block_size_lr,\n              upsample_y=False,use_bias=self.bias,dropout_rate=self.drop)\n        self.stage_1_input_catconv_1 = nn.Conv2d(4*self.features,2*self.features,kernel_size=(1,1),bias=self.bias)\n        self.stage_1_encoder_block_1 = UNetEncoderBlock(cin = 3*self.features, num_channels=2*self.features, \n                block_size=self.block_size_hr if 1 < self.high_res_stages else self.block_size_lr, \n                grid_size=self.grid_size_hr if 1 < self.high_res_stages else self.block_size_lr, \n                lrelu_slope=self.lrelu_slope, block_gmlp_factor=self.block_gmlp_factor, grid_gmlp_factor=self.grid_gmlp_factor,\n                input_proj_factor=self.input_proj_factor, channels_reduction=self.channels_reduction, dropout_rate=self.drop, use_bias=self.bias,\n                downsample=True,use_global_mlp=self.use_global_mlp)\n        #depth=2\n        self.stage_1_input_conv_2 = nn.Conv2d(3,4*self.features,kernel_size=(3,3),bias=self.bias,padding=1)\n        self.stage_1_input_fuse_sam_2 = CrossGatingBlock(x_features=4*self.features,num_channels=4*self.features,\n              block_size=self.block_size_hr if 2 < self.high_res_stages else self.block_size_lr,\n              grid_size=self.grid_size_hr if 2 < self.high_res_stages else self.block_size_lr,\n              upsample_y=False,use_bias=self.bias,dropout_rate=self.drop)\n        self.stage_1_input_catconv_2 = nn.Conv2d(4*self.features,4*self.features,kernel_size=(1,1),bias=self.bias)\n        self.stage_1_encoder_block_2 = UNetEncoderBlock(cin=6*self.features, num_channels=4*self.features, \n                block_size=self.block_size_hr if 2 < self.high_res_stages else self.block_size_lr, \n                grid_size=self.grid_size_hr if 2 < self.high_res_stages else self.block_size_lr, \n                lrelu_slope=self.lrelu_slope, block_gmlp_factor=self.block_gmlp_factor, grid_gmlp_factor=self.grid_gmlp_factor,\n                input_proj_factor=self.input_proj_factor, channels_reduction=self.channels_reduction, dropout_rate=self.drop, use_bias=self.bias,\n                downsample=True,use_global_mlp=self.use_global_mlp)\n\n        #bottleneck\n        self.stage_1_global_block_0 = BottleneckBlock(block_size=self.block_size_lr,grid_size=self.grid_size_lr,features=4 * self.features,\n            block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,input_proj_factor=self.input_proj_factor,\n            dropout_rate=self.drop,use_bias=self.bias,channels_reduction=self.channels_reduction)\n        self.stage_1_global_block_1 = BottleneckBlock(block_size=self.block_size_lr,grid_size=self.grid_size_lr,features=4 * self.features,\n            block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,input_proj_factor=self.input_proj_factor,\n            dropout_rate=self.drop,use_bias=self.bias,channels_reduction=self.channels_reduction)\n\n        #cross gating\n        #depth=2\n        self.UpSampleRatio_18 = UpSampleRatio_4(1*self.features,ratio=2**(-2),use_bias=self.bias)  #0->2\n        self.UpSampleRatio_19 = UpSampleRatio_2(2 * self.features,ratio=2**(-1),use_bias=self.bias)#1->2\n        self.UpSampleRatio_20 = UpSampleRatio(4 * self.features,ratio=1,use_bias=self.bias)#2->2\n        self.stage_1_cross_gating_block_2 = CrossGatingBlock(cin_y=4*features,x_features=3*(2**2) * self.features, num_channels=4*features,\n              block_size=self.block_size_hr if 2 < self.high_res_stages else self.block_size_lr,\n              grid_size=self.grid_size_hr if 2 < self.high_res_stages else self.block_size_lr,\n              upsample_y=True,use_bias=self.bias,dropout_rate=self.drop)\n        #depth=1\n        self.UpSampleRatio_21 = UpSampleRatio_2(1 * self.features,ratio=2**(-1),use_bias=self.bias)#0->1\n        self.UpSampleRatio_22 = UpSampleRatio(2 * self.features,ratio=2**(0),use_bias=self.bias)#1->1\n        self.UpSampleRatio_23 = UpSampleRatio_1_2(4 * self.features,ratio=2,use_bias=self.bias)#2->1\n        self.stage_1_cross_gating_block_1 = CrossGatingBlock(cin_y=4*features,x_features=3*2 * self.features,num_channels=2*features,\n              block_size=self.block_size_hr if 1 < self.high_res_stages else self.block_size_lr,\n              grid_size=self.grid_size_hr if 1 < self.high_res_stages else self.block_size_lr,\n              upsample_y=True,use_bias=self.bias,dropout_rate=self.drop)\n        #depth=0\n        self.UpSampleRatio_24 = UpSampleRatio(1 * self.features,ratio=1,use_bias=self.bias)#0->0\n        self.UpSampleRatio_25 = UpSampleRatio_1_2(2 * self.features,ratio=2,use_bias=self.bias)#1->0\n        self.UpSampleRatio_26 = UpSampleRatio_1_4(4 * self.features,ratio=4,use_bias=self.bias)#2->0\n        self.stage_1_cross_gating_block_0 = CrossGatingBlock(cin_y=2*features,x_features=3 * self.features,num_channels=self.features,\n              block_size=self.block_size_hr if 0 < self.high_res_stages else self.block_size_lr,\n              grid_size=self.grid_size_hr if 0 < self.high_res_stages else self.block_size_lr,\n              upsample_y=True,use_bias=self.bias,dropout_rate=self.drop)\n\n        #decoder\n        #depth=2\n        self.UpSampleRatio_27 = UpSampleRatio(4 * self.features,ratio=2**(0),use_bias=self.bias)#2->2\n        self.UpSampleRatio_28 = UpSampleRatio_2(2 * self.features,ratio=2**(-1),use_bias=self.bias)#1->2\n        self.UpSampleRatio_29 = UpSampleRatio_4(1 * self.features,ratio=2**(-2),use_bias=self.bias)#0->2\n        self.stage_1_decoder_block_2 = UNetDecoderBlock(cin=4*self.features, num_channels=(2**2)*self.features,lrelu_slope=self.lrelu_slope,\n            block_size=self.block_size_hr if 2 < self.high_res_stages else self.block_size_lr,\n            grid_size=self.grid_size_hr if 2 < self.high_res_stages else self.block_size_lr,\n            block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,input_proj_factor=self.input_proj_factor,\n            channels_reduction=self.channels_reduction,use_global_mlp=self.use_global_mlp,dropout_rate=self.drop,use_bias=self.bias)\n        self.stage_1_supervised_attention_module_2 = SAM(features=2 ** (2)*self.features,output_channels=self.num_outputs,use_bias=self.bias)\n        #depth=1\n        self.UpSampleRatio_30 = UpSampleRatio_1_2(4 * self.features,ratio=2**(1),use_bias=self.bias)#2->1\n        self.UpSampleRatio_31 = UpSampleRatio(2 * self.features,ratio=2**(0),use_bias=self.bias)#1->1\n        self.UpSampleRatio_32 = UpSampleRatio_2(1 * self.features,ratio=2**(-1),use_bias=self.bias)#0->1\n        self.stage_1_decoder_block_1 = UNetDecoderBlock(cin=4*self.features, num_channels=(2**1)*self.features,lrelu_slope=self.lrelu_slope,\n            block_size=self.block_size_hr if 1 < self.high_res_stages else self.block_size_lr,\n            grid_size=self.grid_size_hr if 1 < self.high_res_stages else self.block_size_lr,\n            block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,input_proj_factor=self.input_proj_factor,\n            channels_reduction=self.channels_reduction,use_global_mlp=self.use_global_mlp,dropout_rate=self.drop,use_bias=self.bias)\n        self.stage_1_supervised_attention_module_1 = SAM(features=2 ** (1)*self.features,output_channels=self.num_outputs,use_bias=self.bias)\n        #depth=0\n        self.UpSampleRatio_33 = UpSampleRatio_1_4(4 * self.features,ratio=4,use_bias=self.bias)#2->0\n        self.UpSampleRatio_34 = UpSampleRatio_1_2(2 * self.features,ratio=2,use_bias=self.bias)#1->0\n        self.UpSampleRatio_35 = UpSampleRatio(1 * self.features,ratio=1,use_bias=self.bias)#0->0\n        self.stage_1_decoder_block_0 = UNetDecoderBlock(cin=2*self.features,num_channels=self.features,lrelu_slope=self.lrelu_slope,\n            block_size=self.block_size_hr if 0 < self.high_res_stages else self.block_size_lr,\n            grid_size=self.grid_size_hr if 0 < self.high_res_stages else self.block_size_lr,\n            block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,input_proj_factor=self.input_proj_factor,\n            channels_reduction=self.channels_reduction,use_global_mlp=self.use_global_mlp,dropout_rate=self.drop,use_bias=self.bias)\n        self.stage_1_supervised_attention_module_0 = SAM(features=2 ** (0)*self.features,output_channels=self.num_outputs,use_bias=self.bias)\n\n        ########## STAGE 2 ##########\n        #multi scale input and encoder \n        #depth=0\n        self.stage_2_input_conv_0 = nn.Conv2d(3,self.features,kernel_size=(3,3),bias=self.bias,padding=1)\n        self.stage_2_input_fuse_sam_0 = CrossGatingBlock(x_features=self.features,num_channels=self.features,\n              block_size=self.block_size_hr if 0 < self.high_res_stages else self.block_size_lr,\n              grid_size=self.grid_size_hr if 0 < self.high_res_stages else self.block_size_lr,\n              upsample_y=False,use_bias=self.bias,dropout_rate=self.drop)\n        self.stage_2_input_catconv_0 = nn.Conv2d(2*self.features,self.features,kernel_size=(1,1),bias=self.bias)\n        self.stage_2_encoder_block_0 = UNetEncoderBlock(cin=2*self.features, num_channels=self.features, \n                block_size=self.block_size_hr if 0 < self.high_res_stages else self.block_size_lr, \n                grid_size=self.grid_size_hr if 0 < self.high_res_stages else self.block_size_lr, \n                lrelu_slope=self.lrelu_slope, block_gmlp_factor=self.block_gmlp_factor, grid_gmlp_factor=self.grid_gmlp_factor,\n                input_proj_factor=self.input_proj_factor, channels_reduction=self.channels_reduction, dropout_rate=self.drop, use_bias=self.bias,\n                downsample=True,use_global_mlp=self.use_global_mlp)\n        #depth=1\n        self.stage_2_input_conv_1 = nn.Conv2d(3,2*self.features,kernel_size=(3,3),bias=self.bias,padding=1)\n        self.stage_2_input_fuse_sam_1 = CrossGatingBlock(x_features=2*self.features,num_channels=2*self.features,\n              block_size=self.block_size_hr if 1 < self.high_res_stages else self.block_size_lr,\n              grid_size=self.grid_size_hr if 1 < self.high_res_stages else self.block_size_lr,\n              upsample_y=False,use_bias=self.bias,dropout_rate=self.drop)\n        self.stage_2_input_catconv_1 = nn.Conv2d(4*self.features,2*self.features,kernel_size=(1,1),bias=self.bias)\n        self.stage_2_encoder_block_1 = UNetEncoderBlock(cin = 3*self.features, num_channels=2*self.features, \n                block_size=self.block_size_hr if 1 < self.high_res_stages else self.block_size_lr, \n                grid_size=self.grid_size_hr if 1 < self.high_res_stages else self.block_size_lr, \n                lrelu_slope=self.lrelu_slope, block_gmlp_factor=self.block_gmlp_factor, grid_gmlp_factor=self.grid_gmlp_factor,\n                input_proj_factor=self.input_proj_factor, channels_reduction=self.channels_reduction, dropout_rate=self.drop, use_bias=self.bias,\n                downsample=True,use_global_mlp=self.use_global_mlp)\n        #depth=2\n        self.stage_2_input_conv_2 = nn.Conv2d(3,4*self.features,kernel_size=(3,3),bias=self.bias,padding=1)\n        self.stage_2_input_fuse_sam_2 = CrossGatingBlock(x_features=4*self.features,num_channels=4*self.features,\n              block_size=self.block_size_hr if 2 < self.high_res_stages else self.block_size_lr,\n              grid_size=self.grid_size_hr if 2 < self.high_res_stages else self.block_size_lr,\n              upsample_y=False,use_bias=self.bias,dropout_rate=self.drop)\n        self.stage_2_input_catconv_2 = nn.Conv2d(4*self.features,4*self.features,kernel_size=(1,1),bias=self.bias)\n        self.stage_2_encoder_block_2 = UNetEncoderBlock(cin=6*self.features, num_channels=4*self.features, \n                block_size=self.block_size_hr if 2 < self.high_res_stages else self.block_size_lr, \n                grid_size=self.grid_size_hr if 2 < self.high_res_stages else self.block_size_lr, \n                lrelu_slope=self.lrelu_slope, block_gmlp_factor=self.block_gmlp_factor, grid_gmlp_factor=self.grid_gmlp_factor,\n                input_proj_factor=self.input_proj_factor, channels_reduction=self.channels_reduction, dropout_rate=self.drop, use_bias=self.bias,\n                downsample=True,use_global_mlp=self.use_global_mlp)\n\n        #bottleneck\n        self.stage_2_global_block_0 = BottleneckBlock(block_size=self.block_size_lr,grid_size=self.grid_size_lr,features=4 * self.features,\n            block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,input_proj_factor=self.input_proj_factor,\n            dropout_rate=self.drop,use_bias=self.bias,channels_reduction=self.channels_reduction)\n        self.stage_2_global_block_1 = BottleneckBlock(block_size=self.block_size_lr,grid_size=self.grid_size_lr,features=4 * self.features,\n            block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,input_proj_factor=self.input_proj_factor,\n            dropout_rate=self.drop,use_bias=self.bias,channels_reduction=self.channels_reduction)\n\n        #cross gating\n        #depth=2\n        self.UpSampleRatio_36 = UpSampleRatio_4(1*self.features,ratio=2**(-2),use_bias=self.bias)  #0->2\n        self.UpSampleRatio_37 = UpSampleRatio_2(2 * self.features,ratio=2**(-1),use_bias=self.bias)#1->2\n        self.UpSampleRatio_38 = UpSampleRatio(4 * self.features,ratio=1,use_bias=self.bias)#2->2\n        self.stage_2_cross_gating_block_2 = CrossGatingBlock(cin_y=4*features,x_features=3*(2**2) * self.features, num_channels=4*features,\n              block_size=self.block_size_hr if 2 < self.high_res_stages else self.block_size_lr,\n              grid_size=self.grid_size_hr if 2 < self.high_res_stages else self.block_size_lr,\n              upsample_y=True,use_bias=self.bias,dropout_rate=self.drop)\n        #depth=1\n        self.UpSampleRatio_39 = UpSampleRatio_2(1 * self.features,ratio=2**(-1),use_bias=self.bias)#0->1\n        self.UpSampleRatio_40 = UpSampleRatio(2 * self.features,ratio=2**(0),use_bias=self.bias)#1->1\n        self.UpSampleRatio_41 = UpSampleRatio_1_2(4 * self.features,ratio=2,use_bias=self.bias)#2->1\n        self.stage_2_cross_gating_block_1 = CrossGatingBlock(cin_y=4*features,x_features=3*2 * self.features,num_channels=2*features,\n              block_size=self.block_size_hr if 1 < self.high_res_stages else self.block_size_lr,\n              grid_size=self.grid_size_hr if 1 < self.high_res_stages else self.block_size_lr,\n              upsample_y=True,use_bias=self.bias,dropout_rate=self.drop)\n        #depth=0\n        self.UpSampleRatio_42 = UpSampleRatio(1 * self.features,ratio=1,use_bias=self.bias)#0->0\n        self.UpSampleRatio_43 = UpSampleRatio_1_2(2 * self.features,ratio=2,use_bias=self.bias)#1->0\n        self.UpSampleRatio_44 = UpSampleRatio_1_4(4 * self.features,ratio=4,use_bias=self.bias)#2->0\n        self.stage_2_cross_gating_block_0 = CrossGatingBlock(cin_y=2*features,x_features=3 * self.features,num_channels=self.features,\n              block_size=self.block_size_hr if 0 < self.high_res_stages else self.block_size_lr,\n              grid_size=self.grid_size_hr if 0 < self.high_res_stages else self.block_size_lr,\n              upsample_y=True,use_bias=self.bias,dropout_rate=self.drop)\n\n        #decoder\n        #depth=2\n        self.UpSampleRatio_45 = UpSampleRatio(4 * self.features,ratio=2**(0),use_bias=self.bias)#2->2\n        self.UpSampleRatio_46 = UpSampleRatio_2(2 * self.features,ratio=2**(-1),use_bias=self.bias)#1->2\n        self.UpSampleRatio_47 = UpSampleRatio_4(1 * self.features,ratio=2**(-2),use_bias=self.bias)#0->2\n        self.stage_2_decoder_block_2 = UNetDecoderBlock(cin=4*self.features, num_channels=(2**2)*self.features,lrelu_slope=self.lrelu_slope,\n            block_size=self.block_size_hr if 2 < self.high_res_stages else self.block_size_lr,\n            grid_size=self.grid_size_hr if 2 < self.high_res_stages else self.block_size_lr,\n            block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,input_proj_factor=self.input_proj_factor,\n            channels_reduction=self.channels_reduction,use_global_mlp=self.use_global_mlp,dropout_rate=self.drop,use_bias=self.bias)\n        self.stage_2_output_conv_2 = nn.Conv2d((2**(2))*self.features,self.num_outputs,kernel_size=(3,3), bias=self.bias,padding=1)\n        #depth=1\n        self.UpSampleRatio_48 = UpSampleRatio_1_2(4 * self.features,ratio=2**(1),use_bias=self.bias)#2->1\n        self.UpSampleRatio_49 = UpSampleRatio(2 * self.features,ratio=2**(0),use_bias=self.bias)#1->1\n        self.UpSampleRatio_50 = UpSampleRatio_2(1 * self.features,ratio=2**(-1),use_bias=self.bias)#0->1\n        self.stage_2_decoder_block_1 = UNetDecoderBlock(cin=4*self.features, num_channels=(2**1)*self.features,lrelu_slope=self.lrelu_slope,\n            block_size=self.block_size_hr if 1 < self.high_res_stages else self.block_size_lr,\n            grid_size=self.grid_size_hr if 1 < self.high_res_stages else self.block_size_lr,\n            block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,input_proj_factor=self.input_proj_factor,\n            channels_reduction=self.channels_reduction,use_global_mlp=self.use_global_mlp,dropout_rate=self.drop,use_bias=self.bias)\n        self.stage_2_output_conv_1 = nn.Conv2d((2**(1))*self.features,self.num_outputs,kernel_size=(3,3), bias=self.bias,padding=1)\n        #depth=0\n        self.UpSampleRatio_51 = UpSampleRatio_1_4(4 * self.features,ratio=4,use_bias=self.bias)#2->0\n        self.UpSampleRatio_52 = UpSampleRatio_1_2(2 * self.features,ratio=2,use_bias=self.bias)#1->0\n        self.UpSampleRatio_53 = UpSampleRatio(1 * self.features,ratio=1,use_bias=self.bias)#0->0\n        self.stage_2_decoder_block_0 = UNetDecoderBlock(cin=2*self.features,num_channels=self.features,lrelu_slope=self.lrelu_slope,\n            block_size=self.block_size_hr if 0 < self.high_res_stages else self.block_size_lr,\n            grid_size=self.grid_size_hr if 0 < self.high_res_stages else self.block_size_lr,\n            block_gmlp_factor=self.block_gmlp_factor,grid_gmlp_factor=self.grid_gmlp_factor,input_proj_factor=self.input_proj_factor,\n            channels_reduction=self.channels_reduction,use_global_mlp=self.use_global_mlp,dropout_rate=self.drop,use_bias=self.bias)\n        self.stage_2_output_conv_0 = nn.Conv2d((2**(0))*self.features,self.num_outputs,kernel_size=(3,3), bias=self.bias,padding=1)\n\n    def forward(self, x):\n        shortcuts = []\n        shortcuts.append(x)  #to store multiscale input images\n        # Get multi-scale input images\n        for i in range(1, self.num_supervision_scales):\n            shortcuts.append(nearest_downsample(x, 1./(2**i)))\n\n        # store outputs from all stages and all scales\n        # Eg, [[(64, 64, 3), (128, 128, 3), (256, 256, 3)],   # Stage-1 outputs\n        #      [(64, 64, 3), (128, 128, 3), (256, 256, 3)],]  # Stage-2 outputs\n        outputs_all = []\n        sam_features, encs_prev, decs_prev = [], [], []  #to next stage\n        \n        #different stages\n        ########## STAGE 0 ##########\n        # Input convolution, get multi-scale input features\n        x_scales = []\n        for i in range(self.num_supervision_scales):\n            if i == 0:\n                x_scale = self.stage_0_input_conv_0(shortcuts[i])\n                x_scales.append(x_scale)\n            elif i==1:\n                x_scale = self.stage_0_input_conv_1(shortcuts[i])\n                x_scales.append(x_scale)\n            else:\n                x_scale = self.stage_0_input_conv_2(shortcuts[i])\n                x_scales.append(x_scale)\n        #start encoder blocks\n        encs = []\n        x = x_scales[0]  #First full-scale input feature\n        # use larger blocksize at high-res stages, vice versa.\n        for i in range(self.depth):\n            x_scale = x_scales[i] if i < self.num_supervision_scales else None\n            enc_prev = None\n            dec_prev = None\n            if i==0:\n                x, bridge = self.stage_0_encoder_block_0(x,skip=x_scale,enc=enc_prev,dec=dec_prev)\n            elif i==1:\n                x, bridge = self.stage_0_encoder_block_1(x,skip=x_scale,enc=enc_prev,dec=dec_prev)\n            else:\n                x, bridge = self.stage_0_encoder_block_2(x,skip=x_scale,enc=enc_prev,dec=dec_prev)\n            encs.append(bridge)\n        # Global MLP bottleneck blocks\n        x = self.stage_0_global_block_0(x)\n        x = self.stage_0_global_block_1(x)\n        \n        # cache global feature for cross-gating\n        global_feature = x\n        \n        # start cross gating. Use multi-scale feature fusion\n        skip_features = []\n        for i in reversed(range(self.depth)):  # 2, 1, 0\n            if i == 2:\n                # get multi-scale skip signals from cross-gating block\n                signal0 = self.UpSampleRatio_0(encs[0])\n                signal1 = self.UpSampleRatio_1(encs[1])\n                signal2 = self.UpSampleRatio_2(encs[2])\n                signal = torch.cat([signal0,signal1,signal2],dim=1)\n                # Use cross-gating to cross modulate features\n                skips, global_feature = self.stage_0_cross_gating_block_2(signal, global_feature)\n                skip_features.append(skips)\n            elif i == 1:\n                # get multi-scale skip signals from cross-gating block\n                signal0 = self.UpSampleRatio_3(encs[0])\n                signal1 = self.UpSampleRatio_4(encs[1])\n                signal2 = self.UpSampleRatio_5(encs[2])\n                signal = torch.cat([signal0, signal1, signal2], dim=1)\n                # Use cross-gating to cross modulate features\n                skips, global_feature = self.stage_0_cross_gating_block_1(signal, global_feature)\n                skip_features.append(skips)\n            elif i == 0:\n                # get multi-scale skip signals from cross-gating block\n                signal0 = self.UpSampleRatio_6(encs[0])\n                signal1 = self.UpSampleRatio_7(encs[1])\n                signal2 = self.UpSampleRatio_8(encs[2])\n                signal = torch.cat([signal0,signal1,signal2],dim=1)\n                # Use cross-gating to cross modulate features\n                skips, global_feature = self.stage_0_cross_gating_block_0(signal, global_feature)\n                skip_features.append(skips)\n        \n        # start decoder. Multi-scale feature fusion of cross-gated features\n        outputs, decs, sam_features = [], [], []\n        for i in reversed(range(self.depth)):\n            if i == 2:\n                # get multi-scale skip signals from cross-gating block\n                signal2 = self.UpSampleRatio_9(skip_features[0])\n                signal1 = self.UpSampleRatio_10(skip_features[1])\n                signal0 = self.UpSampleRatio_11(skip_features[2])\n                signal = torch.cat([signal2, signal1, signal0], dim=1)\n                # Decoder block\n                x = self.stage_0_decoder_block_2(x, bridge=signal)\n                decs.append(x)\n                sam, output = self.stage_0_supervised_attention_module_2(x, shortcuts[i])\n                outputs.append(output)\n                sam_features.append(sam)\n            elif i == 1:\n                # get multi-scale skip signals from cross-gating block\n                signal2 = self.UpSampleRatio_12(skip_features[0])\n                signal1 = self.UpSampleRatio_13(skip_features[1])\n                signal0 = self.UpSampleRatio_14(skip_features[2])\n                signal = torch.cat([signal2, signal1, signal0], dim=1)\n                # Decoder block\n                x = self.stage_0_decoder_block_1(x, bridge=signal)\n                decs.append(x)\n                sam, output = self.stage_0_supervised_attention_module_1(x, shortcuts[i])\n                outputs.append(output)\n                sam_features.append(sam)\n            elif i == 0:\n                # get multi-scale skip signals from cross-gating block\n                signal2 = self.UpSampleRatio_15(skip_features[0])\n                signal1 = self.UpSampleRatio_16(skip_features[1])\n                signal0 = self.UpSampleRatio_17(skip_features[2])\n                signal = torch.cat([signal2, signal1, signal0], dim=1)\n                # Decoder block\n                x = self.stage_0_decoder_block_0(x, bridge=signal)\n                decs.append(x)\n                sam, output = self.stage_0_supervised_attention_module_0(x, shortcuts[i])\n                outputs.append(output)\n                sam_features.append(sam)\n                        \n        # Cache encoder and decoder features for later-stage's usage\n        encs_prev = encs[::-1]\n        decs_prev = decs\n        # Store outputs\n        outputs_all.append(outputs)\n\n        ########## STAGE 1 ##########\n        x_scales = []\n        for i in range(self.num_supervision_scales):\n            if i == 0:\n                x_scale = self.stage_1_input_conv_0(shortcuts[i])\n                # If later stages, fuse input features with SAM features from prev stage\n                x_scale, _ = self.stage_1_input_fuse_sam_0(x_scale, sam_features.pop())\n                x_scales.append(x_scale)\n            elif i==1:\n                x_scale = self.stage_1_input_conv_1(shortcuts[i])\n                # If later stages, fuse input features with SAM features from prev stage\n                x_scale, _ = self.stage_1_input_fuse_sam_1(x_scale, sam_features.pop())\n                x_scales.append(x_scale)\n            else:\n                x_scale = self.stage_1_input_conv_2(shortcuts[i])\n                # If later stages, fuse input features with SAM features from prev stage\n                x_scale, _ = self.stage_1_input_fuse_sam_2(x_scale, sam_features.pop())\n                x_scales.append(x_scale)\n        #start encoder blocks\n        encs = []\n        x = x_scales[0]  #First full-scale input feature\n        # use larger blocksize at high-res stages, vice versa.\n        for i in range(self.depth):\n            x_scale = x_scales[i] if i < self.num_supervision_scales else None\n            enc_prev = encs_prev.pop()\n            dec_prev = decs_prev.pop()\n            if i==0:\n                x, bridge = self.stage_1_encoder_block_0(x,skip=x_scale,enc=enc_prev,dec=dec_prev)\n            elif i==1:\n                x, bridge = self.stage_1_encoder_block_1(x,skip=x_scale,enc=enc_prev,dec=dec_prev)\n            else:\n                x, bridge = self.stage_1_encoder_block_2(x,skip=x_scale,enc=enc_prev,dec=dec_prev)\n            encs.append(bridge)\n        # Global MLP bottleneck blocks\n        x = self.stage_1_global_block_0(x)\n        x = self.stage_1_global_block_1(x)\n        \n        # cache global feature for cross-gating\n        global_feature = x\n        \n        # start cross gating. Use multi-scale feature fusion\n        skip_features = []\n        for i in reversed(range(self.depth)):  # 2, 1, 0\n            if i == 2:\n                # get multi-scale skip signals from cross-gating block\n                signal0 = self.UpSampleRatio_18(encs[0])\n                signal1 = self.UpSampleRatio_19(encs[1])\n                signal2 = self.UpSampleRatio_20(encs[2])\n                signal = torch.cat([signal0,signal1,signal2],dim=1)\n                # Use cross-gating to cross modulate features\n                skips, global_feature = self.stage_1_cross_gating_block_2(signal, global_feature)\n                skip_features.append(skips)\n            elif i == 1:\n                # get multi-scale skip signals from cross-gating block\n                signal0 = self.UpSampleRatio_21(encs[0])\n                signal1 = self.UpSampleRatio_22(encs[1])\n                signal2 = self.UpSampleRatio_23(encs[2])\n                signal = torch.cat([signal0, signal1, signal2], dim=1)\n                # Use cross-gating to cross modulate features\n                skips, global_feature = self.stage_1_cross_gating_block_1(signal, global_feature)\n                skip_features.append(skips)\n            elif i == 0:\n                # get multi-scale skip signals from cross-gating block\n                signal0 = self.UpSampleRatio_24(encs[0])\n                signal1 = self.UpSampleRatio_25(encs[1])\n                signal2 = self.UpSampleRatio_26(encs[2])\n                signal = torch.cat([signal0,signal1,signal2],dim=1)\n                # Use cross-gating to cross modulate features\n                skips, global_feature = self.stage_1_cross_gating_block_0(signal, global_feature)\n                skip_features.append(skips)\n        \n        # start decoder. Multi-scale feature fusion of cross-gated features\n        outputs, decs, sam_features = [], [], []\n        for i in reversed(range(self.depth)):\n            if i == 2:\n                # get multi-scale skip signals from cross-gating block\n                signal2 = self.UpSampleRatio_27(skip_features[0])\n                signal1 = self.UpSampleRatio_28(skip_features[1])\n                signal0 = self.UpSampleRatio_29(skip_features[2])\n                signal = torch.cat([signal2, signal1, signal0], dim=1)\n                # Decoder block\n                x = self.stage_1_decoder_block_2(x, bridge=signal)\n                decs.append(x)\n                # output conv, if not final stage, use supervised-attention-block.\n                # not last stage, apply SAM\n                sam, output = self.stage_1_supervised_attention_module_2(x, shortcuts[i])\n                outputs.append(output)\n                sam_features.append(sam)\n            elif i == 1:\n                # get multi-scale skip signals from cross-gating block\n                signal2 = self.UpSampleRatio_30(skip_features[0])\n                signal1 = self.UpSampleRatio_31(skip_features[1])\n                signal0 = self.UpSampleRatio_32(skip_features[2])\n                signal = torch.cat([signal2, signal1, signal0], dim=1)\n                # Decoder block\n                x = self.stage_1_decoder_block_1(x, bridge=signal)\n                decs.append(x)\n                # output conv, if not final stage, use supervised-attention-block.\n                # not last stage, apply SAM\n                sam, output = self.stage_1_supervised_attention_module_1(x, shortcuts[i])\n                outputs.append(output)\n                sam_features.append(sam)\n            elif i == 0:\n                # get multi-scale skip signals from cross-gating block\n                signal2 = self.UpSampleRatio_33(skip_features[0])\n                signal1 = self.UpSampleRatio_34(skip_features[1])\n                signal0 = self.UpSampleRatio_35(skip_features[2])\n                signal = torch.cat([signal2, signal1, signal0], dim=1)\n                # Decoder block\n                x = self.stage_1_decoder_block_0(x, bridge=signal)\n                decs.append(x)\n                # output conv, if not final stage, use supervised-attention-block.\n                # not last stage, apply SAM\n                sam, output = self.stage_1_supervised_attention_module_0(x, shortcuts[i])\n                outputs.append(output)\n                sam_features.append(sam)\n\n        # Cache encoder and decoder features for later-stage's usage\n        encs_prev = encs[::-1]\n        decs_prev = decs\n        # Store outputs\n        outputs_all.append(outputs)\n\n        \n        ########## STAGE 2 ##########\n        x_scales = []\n        for i in range(self.num_supervision_scales):\n            if i == 0:\n                x_scale = self.stage_2_input_conv_0(shortcuts[i])\n                # If later stages, fuse input features with SAM features from prev stage\n                x_scale, _ = self.stage_2_input_fuse_sam_0(x_scale, sam_features.pop())\n                x_scales.append(x_scale)\n            elif i==1:\n                x_scale = self.stage_2_input_conv_1(shortcuts[i])\n                # If later stages, fuse input features with SAM features from prev stage\n                x_scale, _ = self.stage_2_input_fuse_sam_1(x_scale, sam_features.pop())\n                x_scales.append(x_scale)\n            else:\n                x_scale = self.stage_2_input_conv_2(shortcuts[i])\n                # If later stages, fuse input features with SAM features from prev stage\n                x_scale, _ = self.stage_2_input_fuse_sam_2(x_scale, sam_features.pop())\n                x_scales.append(x_scale)\n        #start encoder blocks\n        encs = []\n        x = x_scales[0]  #First full-scale input feature\n        # use larger blocksize at high-res stages, vice versa.\n        for i in range(self.depth):\n            x_scale = x_scales[i] if i < self.num_supervision_scales else None\n            enc_prev = encs_prev.pop()\n            dec_prev = decs_prev.pop()\n            if i==0:\n                x, bridge = self.stage_2_encoder_block_0(x,skip=x_scale,enc=enc_prev,dec=dec_prev)\n            elif i==1:\n                x, bridge = self.stage_2_encoder_block_1(x,skip=x_scale,enc=enc_prev,dec=dec_prev)\n            else:\n                x, bridge = self.stage_2_encoder_block_2(x,skip=x_scale,enc=enc_prev,dec=dec_prev)\n            encs.append(bridge)\n        # Global MLP bottleneck blocks\n        x = self.stage_2_global_block_0(x)\n        x = self.stage_2_global_block_1(x)\n        \n        # cache global feature for cross-gating\n        global_feature = x\n        \n        # start cross gating. Use multi-scale feature fusion\n        skip_features = []\n        for i in reversed(range(self.depth)):  # 2, 1, 0\n            if i == 2:\n                # get multi-scale skip signals from cross-gating block\n                signal0 = self.UpSampleRatio_36(encs[0])\n                signal1 = self.UpSampleRatio_37(encs[1])\n                signal2 = self.UpSampleRatio_38(encs[2])\n                signal = torch.cat([signal0,signal1,signal2],dim=1)\n                # Use cross-gating to cross modulate features\n                skips, global_feature = self.stage_2_cross_gating_block_2(signal, global_feature)\n                skip_features.append(skips)\n            elif i == 1:\n                # get multi-scale skip signals from cross-gating block\n                signal0 = self.UpSampleRatio_39(encs[0])\n                signal1 = self.UpSampleRatio_40(encs[1])\n                signal2 = self.UpSampleRatio_41(encs[2])\n                signal = torch.cat([signal0, signal1, signal2], dim=1)\n                # Use cross-gating to cross modulate features\n                skips, global_feature = self.stage_2_cross_gating_block_1(signal, global_feature)\n                skip_features.append(skips)\n            elif i == 0:\n                # get multi-scale skip signals from cross-gating block\n                signal0 = self.UpSampleRatio_42(encs[0])\n                signal1 = self.UpSampleRatio_43(encs[1])\n                signal2 = self.UpSampleRatio_44(encs[2])\n                signal = torch.cat([signal0,signal1,signal2],dim=1)\n                # Use cross-gating to cross modulate features\n                skips, global_feature = self.stage_2_cross_gating_block_0(signal, global_feature)\n                skip_features.append(skips)\n        \n        # start decoder. Multi-scale feature fusion of cross-gated features\n        outputs = []\n        for i in reversed(range(self.depth)):\n            if i == 2:\n                # get multi-scale skip signals from cross-gating block\n                signal2 = self.UpSampleRatio_45(skip_features[0])\n                signal1 = self.UpSampleRatio_46(skip_features[1])\n                signal0 = self.UpSampleRatio_47(skip_features[2])\n                signal = torch.cat([signal2, signal1, signal0], dim=1)\n                # Decoder block\n                x = self.stage_2_decoder_block_2(x, bridge=signal)\n                decs.append(x)\n                # output conv, if not final stage, use supervised-attention-block.\n                # Last stage, apply output convolutions\n                output = self.stage_2_output_conv_2(x)\n                output = output + shortcuts[i]\n                outputs.append(output)\n            elif i == 1:\n                # get multi-scale skip signals from cross-gating block\n                signal2 = self.UpSampleRatio_48(skip_features[0])\n                signal1 = self.UpSampleRatio_49(skip_features[1])\n                signal0 = self.UpSampleRatio_50(skip_features[2])\n                signal = torch.cat([signal2, signal1, signal0], dim=1)\n                # Decoder block\n                x = self.stage_2_decoder_block_1(x, bridge=signal)\n                decs.append(x)\n                # output conv, if not final stage, use supervised-attention-block.\n                # Last stage, apply output convolutions\n                output = self.stage_2_output_conv_1(x)\n                output = output + shortcuts[i]\n                outputs.append(output)\n            elif i == 0:\n                # get multi-scale skip signals from cross-gating block\n                signal2 = self.UpSampleRatio_51(skip_features[0])\n                signal1 = self.UpSampleRatio_52(skip_features[1])\n                signal0 = self.UpSampleRatio_53(skip_features[2])\n                signal = torch.cat([signal2, signal1, signal0], dim=1)\n                # Decoder block\n                x = self.stage_2_decoder_block_0(x, bridge=signal)\n                decs.append(x)\n                # output conv, if not final stage, use supervised-attention-block.\n                # Last stage, apply output convolutions\n                output = self.stage_2_output_conv_0(x)\n                output = output + shortcuts[i]\n                outputs.append(output)\n\n        # Store outputs\n        outputs_all.append(outputs)\n\n        return outputs_all\n\n\ndef recover_tree(keys, values):\n    \"\"\"Recovers a tree as a nested dict from flat names and values.\n    This function is useful to analyze checkpoints that are saved by our programs\n    without need to access the exact source code of the experiment. In particular,\n    it can be used to extract an reuse various subtrees of the scheckpoint, e.g.\n    subtree of parameters.\n    Args:\n      keys: a list of keys, where '/' is used as separator between nodes.\n      values: a list of leaf values.\n    Returns:\n      A nested tree-like dict.\n    \"\"\"\n    tree = {}\n    sub_trees = collections.defaultdict(list)\n    for k, v in zip(keys, values):\n        if \"/\" not in k:\n            tree[k] = v\n        else:\n            k_left, k_right = k.split(\"/\", 1)\n            sub_trees[k_left].append((k_right, v))\n    for k, kv_pairs in sub_trees.items():\n        k_subtree, v_subtree = zip(*kv_pairs)\n        tree[k] = recover_tree(k_subtree, v_subtree)\n    return tree\n\ndef get_params(ckpt_path):\n    \"\"\"Get params checkpoint.\"\"\"\n    with tf.io.gfile.GFile(ckpt_path, \"rb\") as f:\n        data = f.read()\n    values = np.load(io.BytesIO(data))\n    params = recover_tree(*zip(*values.items()))\n    params = params[\"opt\"][\"target\"]\n    return params\n\ndef modify_jax_params(flat_jax_dict):\n    modified_dict = {}\n    for key, value in flat_jax_dict.items():\n        key_split = key.split(\"/\")\n        modified_value = torch.tensor(value, dtype=torch.float)\n        \n\n        #modify values\n        num_dim = len(modified_value.shape)\n        if num_dim == 1:\n            modified_value = modified_value.squeeze()\n        elif num_dim == 2 and key_split[-1] == 'kernel':\n            # for normal weight, transpose it\n            modified_value = modified_value.T\n        elif num_dim == 4 and key_split[-1] == 'kernel':\n            modified_value = modified_value.permute(3, 2, 0, 1)\n            if num_dim ==4 and key_split[-2] == 'ConvTranspose_0' and key_split[-1] == 'kernel':\n                modified_value = modified_value.permute(1, 0, 2, 3)\n\n\n        #modify keys\n        modified_key = (\".\".join(key_split[:]))\n        if \"kernel\" in modified_key:\n            modified_key = modified_key.replace(\"kernel\", \"weight\")\n        if \"LayerNorm\" in modified_key:\n            modified_key = modified_key.replace(\"scale\", \"gamma\")\n            modified_key = modified_key.replace(\"bias\", \"beta\")\n        if \"layernorm\" in modified_key:\n            modified_key = modified_key.replace(\"scale\", \"gamma\")\n            modified_key = modified_key.replace(\"bias\", \"beta\")\n\n        modified_dict[modified_key] = modified_value\n\n    return modified_dict\n\n\ndef main(args):\n    jax_params = get_params(args.ckpt_path)\n    [flat_jax_dict] = pd.json_normalize(jax_params, sep=\"/\").to_dict(orient=\"records\")  #set separation sign\n\n    # Amend the JAX variables to match the names of the torch variables.\n    modified_jax_params = modify_jax_params(flat_jax_dict)\n\n    # update and save\n    model = MAXIM_dns_3s()\n    maxim_dict = model.state_dict()\n    maxim_dict.update(modified_jax_params)\n    torch.save(maxim_dict, args.output_file)\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\"Conversion of the JAX pre-trained MAXIM weights to Pytorch.\"\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--ckpt_path\",\n        default=\"maxim_ckpt_Denoising_SIDD_checkpoint.npz\",\n        type=str,\n        help=\"Checkpoint to port.\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output_file\",\n        default=\"torch_weight.pth\",\n        type=str,\n        help=\"Output.\",\n    )\n    return parser.parse_args()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax_params = get_params('/kaggle/input/maxim-weights/maxim_ckpt_Deblurring_RealBlur_R_checkpoint.npz')\n[flat_jax_dict] = pd.json_normalize(jax_params, sep=\"/\").to_dict(orient=\"records\")  #set separation sign\n\n# Amend the JAX variables to match the names of the torch variables.\nmodified_jax_params = modify_jax_params(flat_jax_dict)\n\n# update and save\nmodel = MAXIM_dns_3s()\nmaxim_dict = model.state_dict()\nmaxim_dict.update(modified_jax_params)\ntorch.save(maxim_dict, 'maxim_ckpt_Deblurring_RealBlur_R_checkpoint.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://panasonic.net/electricworks/lighting/en_my/case/suita/img/hero04.jpg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://nfsmods.xyz/uploads/thumbnails/nlgxzef-303005a757e6ec0c7df28490bfcf5dda.jpg -O im1.jpg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://i.stack.imgur.com/4blja.png -O im2.png","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = cv2.imread('im2.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_img = torch.tensor(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_img.size()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MAXIM_dns_3s()\nweights = torch.load('maxim_ckpt_Deblurring_RealBlur_R_checkpoint.pth')\nmodel.load_state_dict(weights)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms as transforms\nmax_size = 256\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((max_size, max_size)),\n    transforms.ToTensor()\n])\n\nresized_tensor = transform(t_img.permute(2, 0, 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = model(resized_tensor.unsqueeze(0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_img = output[-1][-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_img.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimg_pil = output_img.squeeze(0).permute(1, 2, 0).detach().cpu().numpy()\n\nfig, ax = plt.subplots()\n\nax.imshow(img_pil)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}